---
title: "SM BigQuery Analyst"
sidebarTitle: "SM BigQuery Analyst"
description: "Query SourceMedium BigQuery safely with SQL receipts. SELECT-only, cost-guarded."
icon: "database"
---

Query SourceMedium-hosted BigQuery data safely with auditable SQL receipts.

## Quick Start (Copy/Paste)

<Tip>
Copy the block below and paste it into your coding agent to install and verify setup automatically.
</Tip>

```
Install the SourceMedium BigQuery analyst skill:

1. Run: npx skills add source-medium/skills@v1.0.1 --skill sm-bigquery-analyst
2. Read the installed SKILL.md to understand the workflow
3. Run the setup verification commands to check my BigQuery access

My BigQuery project ID is: [YOUR_PROJECT_ID]
```

---

## Install

```bash
npx skills add source-medium/skills@v1.0.1 --skill sm-bigquery-analyst
```

<Note>
We recommend pinning to a specific version (`@v1.0.0`) to avoid surprise behavior changes.
</Note>

Or copy the SKILL.md content below into your coding agent's skills folder.

## What this skill does

1. **Setup verification** — validates gcloud/bq CLI, authentication, and table access
2. **Safe queries** — SELECT-only, dry-run first, cost-guarded
3. **SQL receipts** — every answer includes copy/paste SQL + verification command
4. **No fabrication** — if access fails, returns exact error and access request template

---

## Example Questions

After installing, ask your coding agent:

```
What was my revenue by channel last month?
```

```
Show me new customer acquisition by source over the past 30 days
```

```
What's my customer LTV by acquisition cohort? Use sm_order_line_type = 'all_orders'
```

Your agent will verify access, generate SQL, and return an auditable receipt.

---

## SKILL.md

Copy everything below into `.claude/skills/sm-bigquery-analyst/SKILL.md` (or equivalent for your agent):

```markdown
---
name: sm-bigquery-analyst
description: Query SourceMedium-hosted BigQuery safely. Emits SQL receipts. SELECT-only, cost-guarded. Use when users need help with BigQuery setup, access verification, or analytical questions against SourceMedium datasets.
compatibility: Requires gcloud CLI, bq CLI, and network access to BigQuery.
metadata:
  author: sourcemedium
  version: "1.0"
---

# SourceMedium BigQuery Analyst

Use this skill to help end users work with SourceMedium BigQuery data from setup to analysis.

## Workflow

1. **Verify environment** (run these before any analysis)
2. Confirm project and dataset/table visibility
3. Use docs-first guidance for definitions and table discovery
4. Answer analytical questions with reproducible SQL receipts
5. Call out assumptions and caveats explicitly

## Setup Verification

Run these commands in order before writing analysis SQL:

~~~bash
# 1. Check CLI tools are installed
gcloud --version && bq version

# 2. Check authenticated account
gcloud auth list

# 3. Check active project
gcloud config get-value project

# 4. Validate BigQuery API access (dry-run)
bq query --use_legacy_sql=false --dry_run 'SELECT 1 AS ok'

# 5. Test table access (replace YOUR_PROJECT_ID)
bq query --use_legacy_sql=false --dry_run "
  SELECT 1 
  FROM \`YOUR_PROJECT_ID.sm_transformed_v2.obt_orders\` 
  LIMIT 1
"
~~~

If any step fails, direct the user to request access from their internal admin.

## Safety Rules

These are hard constraints. Do not bypass.

### Query Safety

1. **SELECT-only** — deny: INSERT, UPDATE, DELETE, MERGE, CREATE, DROP, EXPORT, COPY
2. **Dry-run first** when iterating on new queries: `bq query --dry_run '...'`
3. **Maximum bytes billed** — warn if scan exceeds 1GB without explicit approval
4. **Always bound queries**:
   - Add `LIMIT` clause (max 100 rows for exploratory)
   - Use date/partition filters when querying partitioned tables
   - Prefer `WHERE` filters on partition columns

### Data Safety

1. **Default to aggregates** — avoid outputting raw rows unless explicitly requested
2. **PII handling**:
   - Do not output columns likely containing PII (email, phone, address, name) without explicit confirmation
   - If PII is requested, confirm scope and purpose before proceeding
   - Suggest anonymization (hashing, aggregation) as alternatives

### Cost Guardrails

~~~sql
-- Good: bounded scan
SELECT ... FROM `project.dataset.table`
WHERE DATE(column) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
LIMIT 100

-- Bad: full table scan
SELECT ... FROM `project.dataset.table`  -- no filters
~~~

## Output Contract

For analytical questions, always return:

1. **Answer** — concise plain-English conclusion
2. **SQL (copy/paste)** — BigQuery Standard SQL used for the result
3. **Notes** — timeframe, metric definitions, grain, scope, timezone, attribution lens
4. **Verify** — `bq query --use_legacy_sql=false --dry_run '<SQL>'` command

If access/setup fails, do not fabricate results. Return:

1. Exact failing step
2. Exact project/dataset that failed
3. Direct user to the BigQuery access request template

## Query Guardrails

1. Fully qualify tables as `` `project.dataset.table` ``
2. For order analyses, default to `WHERE is_order_sm_valid = TRUE`
3. Use `sm_store_id` (not `smcid` — that name does not exist in customer tables)
4. Use `SAFE_DIVIDE` for ratio math
5. Handle DATE/TIMESTAMP typing explicitly (`DATE(ts_col)` when comparing to dates)
6. Use `order_net_revenue` for revenue metrics (not `order_gross_revenue` unless explicitly asked)
7. Use `*_local_datetime` columns for date-based reporting (not UTC `*_at` columns)
8. Avoid `LIKE`/`REGEXP` on low-cardinality fields; discover values first with `SELECT DISTINCT`, then use exact match
9. `LIKE` is acceptable for free-text fields (`utm_campaign`, `product_title`, `page_path`)
10. **LTV tables (`rpt_cohort_ltv_*`)**: always filter `sm_order_line_type` to exactly ONE value

## Key Tables

| Table | Grain | Use case |
|-------|-------|----------|
| `obt_orders` | 1 row per order | Revenue, profitability, channel analysis |
| `obt_order_lines` | 1 row per line item | Product performance, margins, COGS |
| `obt_customers` | 1 row per customer | Acquisition, retention, subscription status |
| `rpt_ad_performance_daily` | 1 row per channel/date | Ad spend, impressions, clicks |
| `rpt_cohort_ltv_*` | 1 row per cohort x month | LTV analysis (filter sm_order_line_type!) |

## Example Queries

### Daily revenue by channel

~~~sql
SELECT
  DATE(order_processed_at_local_datetime) AS order_date,
  sm_channel,
  COUNT(sm_order_key) AS order_count,
  SUM(order_net_revenue) AS revenue
FROM `your_project.sm_transformed_v2.obt_orders`
WHERE is_order_sm_valid = TRUE
  AND DATE(order_processed_at_local_datetime) >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
GROUP BY 1, 2
ORDER BY 1 DESC
~~~

### New customer acquisition

~~~sql
SELECT
  DATE(order_processed_at_local_datetime) AS order_date,
  sm_utm_source_medium,
  COUNT(DISTINCT sm_customer_key) AS new_customers,
  SUM(order_net_revenue) AS revenue
FROM `your_project.sm_transformed_v2.obt_orders`
WHERE is_order_sm_valid = TRUE
  AND order_sequence = '1st_order'
GROUP BY 1, 2
ORDER BY 1 DESC
~~~

### LTV cohort (CRITICAL: filter sm_order_line_type)

~~~sql
SELECT
  cohort_month,
  months_since_first_order,
  AVG(SAFE_DIVIDE(cumulative_order_net_revenue, cohort_size)) AS avg_ltv
FROM `your_project.sm_transformed_v2.rpt_cohort_ltv_by_first_valid_purchase_attribute_no_product_filters`
WHERE sm_order_line_type = 'all_orders'
  AND months_since_first_order <= 12
GROUP BY 1, 2
ORDER BY 1, 2
~~~
```

---

## No access yet?

If you cannot run queries due to permissions, see [BigQuery Access Request Template](/ai-analyst/agent-skills/bigquery-access-request-template).

## Related docs

<CardGroup cols={2}>
  <Card title="BigQuery Essentials" icon="database" href="/onboarding/analytics-tools/bigquery-essentials">
    Setup and first-query fundamentals.
  </Card>
  <Card title="SQL Query Library" icon="code" href="/data-activation/template-resources/sql-query-library">
    SourceMedium SQL templates and patterns.
  </Card>
  <Card title="Table Docs" icon="table" href="/data-activation/data-tables/sm_transformed_v2/index">
    Schema-level documentation for core tables.
  </Card>
  <Card title="Multi-Touch Attribution" icon="chart-network" href="/mta/mta-overview">
    MTA models and experimental tables.
  </Card>
</CardGroup>
